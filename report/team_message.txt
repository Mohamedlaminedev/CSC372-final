Hey team,

We’ve got four specs from the CSC 372 final (swap, descending flag, left rotation, transform). For each one, we need two C versions: a “good” one that meets the spec and a “bad” one that fails. The trick is we’re working with Frama-C and ACSL, so every file has the spec at the top, the code below, and we run `frama-c -wp <file>.c -then -report` to see if it passes.

What each of us has to do for our assigned spec:
1. Copy the given ACSL spec and C stub (already in `src/`).
2. Prompt the LLM (same wording each time) to fill in the function body. Save the full conversation with timestamps, model, and settings.
3. Run Frama-C on the code—if it succeeds, that’s the “good” version. Run the same prompt again (maybe different temp) until you get a version that Frama-C rejects—keep that as the “bad” version. Document both runs.
4. Drop the code and Frama-C output into the repo (one file/table per version) and save the transcript in `transcripts/`.
5. Write a short paragraph describing what you attempted for your spec and why one version passed/failed.

Final deliverable is one PDF with all four specs (8 snippets total) + transcripts + Frama-C reports + our names, due Dec 10.

Possible split (open to changes):
- Person A: P1 swap good/bad
- Person B: P2 descending flag good/bad
- Person C: P3 left rotation good/bad
- Person D: P4 transform good/bad

If anyone wants to stick with verification instead of prompting, we can pair up: one person runs prompts, the other handles Frama-C and report write-up for that spec.

Let me know which part you want so we can start filling in the `src/` files and collecting transcripts. I’ll keep everything organized in the GitHub repo so we can merge it into the final PDF later.

Thanks!
Mohamed
